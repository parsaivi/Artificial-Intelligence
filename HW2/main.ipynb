{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d7c62df",
   "metadata": {},
   "source": [
    "<font>\n",
    "<div dir=ltr align=center>\n",
    "<img src='https://cdn.freebiesupply.com/logos/large/2x/sharif-logo-png-transparent.png' width=150 height=150> <br>\n",
    "<font color=0F5298 size=7>\n",
    "Artificial Intelligence <br>\n",
    "<font color=2565AE size=5>\n",
    "Computer Engineering Department <br>\n",
    "Spring 2025<br>\n",
    "<font color=3C99D size=5>\n",
    "Practical Assignment 2 - Minimax, MDP and RL <br>\n",
    "<font color=696880 size=4>\n",
    "Arman Tahmasebi Zadeh\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ab602e",
   "metadata": {},
   "source": [
    "Welcome! This notebook is all about strategy—or, as some might call it, \"policy.\" Choosing the best action during a game or while solving a real-world problem is one of the most important applications of AI today. In fact, it could even be considered a core definition of intelligence. Here, you’ll explore several approaches to tackling this challenge, each with its own strengths and weaknesses.\n",
    "\n",
    "For now, let’s start by importing the necessary tools. Feel free to add additional imports if you like, though it's unlikely you'll need more than what's provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7834c8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gymnasium import Env, spaces\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import imageio\n",
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "import gymnasium as gym\n",
    "import random\n",
    "import os\n",
    "from PIL import Image, ImageDraw\n",
    "import math\n",
    "import matplotlib.animation as animation\n",
    "import networkx as nx\n",
    "from numpy.linalg import norm\n",
    "import copy\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ec0469",
   "metadata": {},
   "source": [
    "Now, to the main part..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7597fcd",
   "metadata": {},
   "source": [
    "# Minimax\n",
    "One classic method we'll encounter is the minimax algorithm, a foundational strategy used primarily in two-player games. Minimax assumes both players act optimally and tries to minimize the possible loss for a worst-case scenario—essentially planning ahead by considering the best moves for yourself and the best counter-moves for your opponent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f536a1b",
   "metadata": {},
   "source": [
    "## Onitama\n",
    "\n",
    "In this section, a simpler version of the board game **Onitama** is implemented. This game has mechanics similar to chess, but with a unique twist: the available moves for each player's pieces change at the end of each round.\n",
    "\n",
    "Onitama is played on a 5x5 board. Each player controls five pieces—one Master and four Students. The objective is to either capture the opponent's Master or move your own Master onto the opponent's temple arch space.\n",
    "\n",
    "At any given time, each player has two movement cards that dictate how their pieces can move. A fifth card sits aside and rotates into play: when a player uses a move from one of their two cards, that card is then swapped with the spare card. This continuous rotation of moves adds a strategic layer, forcing players to think not just about their own plans, but also about what moves they are giving to their opponent.\n",
    "\n",
    "In the simplified version implemented here, we focus on the core movement and card-swapping mechanics, while leaving out some of the more subtle complexities of the full game.\n",
    "\n",
    "\n",
    "\n",
    "![Onitama Board Game](Onitama.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2507dc",
   "metadata": {},
   "source": [
    "### Simplifications\n",
    "In the implemented version, some rules are different from the main game:\n",
    "- The only way to win is to capture all of your opponent's pieces.\n",
    "- Each player has only one move card.\n",
    "- The size of the board can vary. The number of pieces on each side is board size - 2.\n",
    "- There is no Master piece, all of the pieces are Students.\n",
    "\n",
    "---\n",
    "\n",
    "### Game Rules\n",
    "Each card in the game represents a set of moves, implemented as an array of tuples. In the card visualization, the center shows your piece's position, and all of the other colored squares show all of the possible moves that can be done using that card. For instance, using COBRA, you can either move one of your pieces left, top-right or bottom-right for one square.\n",
    "\n",
    "At the start of each round:\n",
    "- The current player selects one of the moves on the card.\n",
    "- He then selects one of his remaining pieces.\n",
    "- The move is applied to the piece, if it is valid, meaning that it does not land on another one of your own pieces or it goes out of border.\n",
    "- If a piece lands on an enemy piece, the enemy piece is destroyed and removed from the board.\n",
    "- If there is a possible move, it must be taken.\n",
    "- If there are no possible moves, the player does nothing and passes his turn.\n",
    "- After the move is done, his card is swapped with the center card.\n",
    "- Current player's turn is over\n",
    "\n",
    "Pay attention to the fact that the cards rotate when swapped, meaning that if I can go two squares forward with TIGER, my opponent can go two squares forward with it as well, with respect to his point of view, which happens to be two squares backward for me.\n",
    "\n",
    "---\n",
    "\n",
    "A sample run of the game is implemented below. You can also go and watch a short video on how to play Onitama to understand better. \n",
    "\n",
    "You don't need to change any parts of the following two cells, although you can add some custom cards to `ALL_CARDS` and play with them instead of the default ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803d9fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_CARDS = {\n",
    "    \"Tiger\": [(0, 2), (0, -1)],\n",
    "    \"Dragon\": [(-2, 1), (2, 1), (-1, -1), (1, -1)],\n",
    "    \"Frog\": [(-2, 0), (-1, 1), (1, -1)],\n",
    "    \"Rabbit\": [(1, 1), (-1, -1), (2, 0)],\n",
    "    \"Crab\": [(-2, 0), (0, 1), (2, 0)],\n",
    "    \"Elephant\": [(-1, 1), (1, 1), (-1, 0), (1, 0)],\n",
    "    \"Goose\": [(-1, 1), (0, 1), (0, -1), (1, -1)],\n",
    "    \"Rooster\": [(1, 1), (0, 1), (0, -1), (-1, -1)],\n",
    "    \"Monkey\": [(-1, 1), (1, 1), (-1, -1), (1, -1)],\n",
    "    \"Mantis\": [(-1, 1), (1, 1), (0, -1)],\n",
    "    \"Horse\": [(0, 1), (-1, 0), (0, -1)],\n",
    "    \"Ox\": [(0, 1), (1, 0), (0, -1)],\n",
    "    \"Crane\": [(0, 1), (-1, -1), (1, -1)],\n",
    "    \"Boar\": [(-1, 0), (0, 1), (1, 0)],\n",
    "    \"Eel\": [(-1, 1), (1, 0), (-1, -1)],\n",
    "    \"Cobra\": [(1, 1), (-1, 0), (1, -1)],\n",
    "    \n",
    "    \"Your Custom Card\": [(10, 10)],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299ebf2d",
   "metadata": {},
   "source": [
    "![Onitama Cards](OnitamaCards.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197fcd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Onitama:\n",
    "    def __init__(\n",
    "            self, \n",
    "            board_size=5, \n",
    "            my_symbol='o', \n",
    "            opponent_symbol='x', \n",
    "            card1=\"Tiger\", \n",
    "            card2=\"Crab\", \n",
    "            card3=\"Elephant\", \n",
    "            figsize=(9, 6), # Changes the animation size\n",
    "            render_interval=800 # Changes the time between frames\n",
    "            ):\n",
    "        self.board_size = board_size\n",
    "        self.my_symbol = my_symbol\n",
    "        self.opponent_symbol = opponent_symbol\n",
    "        self.card1 = card1\n",
    "        self.card2 = card2\n",
    "        self.card3 = card3\n",
    "        self.figsize = figsize\n",
    "        self.interval = render_interval\n",
    "        \n",
    "        m = 0\n",
    "        for c in [ALL_CARDS.get(card1), ALL_CARDS.get(card2), ALL_CARDS.get(card3)]:\n",
    "            for a, b in c:\n",
    "                if abs(a) > m:\n",
    "                    m = abs(a)\n",
    "                if abs(b) > m:\n",
    "                    m = abs(b)\n",
    "        self.max_move = m\n",
    "\n",
    "    def _init_board(self):\n",
    "        board = [['.' for _ in range(self.board_size)] for _ in range(self.board_size)]\n",
    "        for i in range(1, self.board_size - 1):\n",
    "            board[0][i] = self.opponent_symbol\n",
    "            board[-1][i] = self.my_symbol\n",
    "        return board\n",
    "\n",
    "    def init_game(self):\n",
    "        return {\n",
    "            \"board\": self._init_board(),\n",
    "            \"turn\": 1,  # 1 = You, -1 = Opponent\n",
    "            \"my_card\": self.card1,\n",
    "            \"opponent_card\": self.card2,\n",
    "            \"center\": self.card3,\n",
    "        }\n",
    "\n",
    "    def get_pieces(self, board, player):\n",
    "        symbol = self.my_symbol if player == 1 else self.opponent_symbol\n",
    "        return [(r, c) for r in range(self.board_size) for c in range(self.board_size) if board[r][c] == symbol]\n",
    "\n",
    "    def apply_move(self, state, from_pos, to_pos):\n",
    "        next_state = copy.deepcopy(state)\n",
    "        r1, c1 = from_pos\n",
    "        r2, c2 = to_pos\n",
    "        piece = next_state[\"board\"][r1][c1]\n",
    "        next_state[\"board\"][r1][c1] = '.'\n",
    "        next_state[\"board\"][r2][c2] = piece\n",
    "\n",
    "        if next_state[\"turn\"] == 1:\n",
    "            next_state[\"my_card\"], next_state[\"center\"] = next_state[\"center\"], next_state[\"my_card\"]\n",
    "        else:\n",
    "            next_state[\"opponent_card\"], next_state[\"center\"] = next_state[\"center\"], next_state[\"opponent_card\"]\n",
    "\n",
    "        next_state[\"turn\"] *= -1\n",
    "        return next_state\n",
    "\n",
    "    def in_bounds(self, r, c):\n",
    "        return 0 <= r < self.board_size and 0 <= c < self.board_size\n",
    "\n",
    "    def generate_moves(self, state):\n",
    "        moves = []\n",
    "        board = state[\"board\"]\n",
    "        player = state[\"turn\"]\n",
    "        card = state[\"my_card\"] if player == 1 else state[\"opponent_card\"]\n",
    "        pieces = self.get_pieces(board, player)\n",
    "\n",
    "        for (r, c) in pieces:\n",
    "            for (dx, dy) in ALL_CARDS[card]:\n",
    "                dc, dr = (-dx, -dy) if player == 1 else (dx, dy)\n",
    "                nr, nc = r + dr, c + dc\n",
    "                if self.in_bounds(nr, nc):\n",
    "                    target = board[nr][nc]\n",
    "                    if player == 1 and target != self.my_symbol:\n",
    "                        moves.append((r, c, nr, nc))\n",
    "                    elif player == -1 and target != self.opponent_symbol:\n",
    "                        moves.append((r, c, nr, nc))\n",
    "        return moves\n",
    "\n",
    "    def evaluate(self, state):\n",
    "        flat = [cell for row in state[\"board\"] for cell in row]\n",
    "        return flat.count(self.my_symbol) - flat.count(self.opponent_symbol)\n",
    "\n",
    "    def terminal(self, state):\n",
    "        if all(cell != self.opponent_symbol for row in state[\"board\"] for cell in row):\n",
    "            return True, 1\n",
    "        if all(cell != self.my_symbol for row in state[\"board\"] for cell in row):\n",
    "            return True, -1\n",
    "        return False, 0\n",
    "\n",
    "    def _render(self, states):\n",
    "        fig, axs = plt.subplots(2, 3, figsize=self.figsize, height_ratios=[1, 4], width_ratios=[1, 4, 1])\n",
    "\n",
    "        ax_opp, ax_center, ax_player = axs[0]\n",
    "        ax_board = axs[1][1]\n",
    "\n",
    "        axs[1][0].axis('off')\n",
    "        axs[1][2].axis('off')\n",
    "\n",
    "        def render_card_moves(ax, card_name, is_opponent, max_move):\n",
    "            s = 2 * max_move + 1\n",
    "            ax.clear()\n",
    "            ax.set_xticks(np.arange(s))\n",
    "            ax.set_yticks(np.arange(s))\n",
    "            ax.set_xticklabels([])\n",
    "            ax.set_yticklabels([])\n",
    "            ax.set_xlim(-0.5, s - 0.5)\n",
    "            ax.set_ylim(-0.5, s - 0.5)\n",
    "            ax.grid(True)\n",
    "            ax.set_aspect('equal')\n",
    "            ax.set_title(card_name, fontsize=10)\n",
    "\n",
    "            origin = (max_move, max_move)\n",
    "            ax.add_patch(plt.Circle(origin, 0.2, color='black'))\n",
    "\n",
    "            for dx, dy in ALL_CARDS.get(card_name, []):\n",
    "                if is_opponent:\n",
    "                    dx, dy = -dx, -dy\n",
    "                x = origin[0] + dx\n",
    "                y = origin[1] + dy\n",
    "                if 0 <= x < s and 0 <= y < s:\n",
    "                    ax.add_patch(plt.Circle((x, y), 0.2, color='blue'))\n",
    "\n",
    "        def set_card_text(ax, card_name, label, color, max_move):\n",
    "            render_card_moves(ax, card_name, label == \"Opponent\", max_move)\n",
    "            ax.set_title(f\"{label}: {card_name}\", fontsize=10, color=color)\n",
    "\n",
    "        def draw_board(state, my_symbol, opponent_symbol, board_size):\n",
    "            ax_board.clear()\n",
    "            ax_board.set_xticks(np.arange(board_size))\n",
    "            ax_board.set_yticks(np.arange(board_size))\n",
    "            ax_board.set_xticklabels([])\n",
    "            ax_board.set_yticklabels([])\n",
    "            ax_board.set_xlim(-0.5, board_size - 0.5)\n",
    "            ax_board.set_ylim(-0.5, board_size - 0.5)\n",
    "            ax_board.grid(True)\n",
    "            ax_board.set_title(\"Onitama\", fontsize=14)\n",
    "\n",
    "            board = state[\"board\"]\n",
    "            for r in range(board_size):\n",
    "                for c in range(board_size):\n",
    "                    piece = board[r][c]\n",
    "                    if piece == my_symbol:\n",
    "                        ax_board.text(c, board_size - 1 - r, my_symbol, ha='center', va='center', fontsize=18, color='blue', weight='bold')\n",
    "                    elif piece == opponent_symbol:\n",
    "                        ax_board.text(c, board_size - 1 - r, opponent_symbol, ha='center', va='center', fontsize=18, color='red', weight='bold')\n",
    "\n",
    "        def update(frame_idx, my_symbol, opponent_symbol, board_size, max_move):\n",
    "            state = states[frame_idx]\n",
    "            draw_board(state, my_symbol, opponent_symbol, board_size)\n",
    "\n",
    "            set_card_text(ax_opp, state.get(\"opponent_card\", \"\"), \"Opponent\", \"red\", max_move)\n",
    "            set_card_text(ax_center, state.get(\"center\", \"\"), \"Center\", \"black\", max_move)\n",
    "            set_card_text(ax_player, state.get(\"my_card\", \"\"), \"You\", \"blue\", max_move)\n",
    "\n",
    "            if \"winner\" in state:\n",
    "                winner = state[\"winner\"]\n",
    "                if winner == 1:\n",
    "                    ax_board.set_title(\"Game Over: Player Wins\", fontsize=14)\n",
    "                elif winner == -1:\n",
    "                    ax_board.set_title(\"Game Over: Opponent Wins\", fontsize=14)\n",
    "                else:\n",
    "                    ax_board.set_title(\"Game Over: Draw\", fontsize=14)\n",
    "\n",
    "        ani = animation.FuncAnimation(fig, partial(update, my_symbol=self.my_symbol, opponent_symbol=self.opponent_symbol, board_size=self.board_size, max_move=self.max_move), frames=len(states), interval=self.interval, repeat=False)\n",
    "        plt.close(fig)\n",
    "        return HTML(ani.to_jshtml())\n",
    "\n",
    "\n",
    "    def run(self, agent1, agent2, render=True):\n",
    "        state = self.init_game()\n",
    "        states = [copy.deepcopy(state)]\n",
    "\n",
    "        while True:\n",
    "            terminal, winner = self.terminal(state)\n",
    "            if terminal:\n",
    "                state[\"winner\"] = winner\n",
    "                states.append(copy.deepcopy(state))\n",
    "                break\n",
    "            \n",
    "            turn = state[\"turn\"] == 1\n",
    "\n",
    "            if turn:\n",
    "                move = agent1.make_move(state)\n",
    "            else:\n",
    "                move = agent2.make_move(state)\n",
    "\n",
    "            if move is None:\n",
    "                state[\"winner\"] = 0\n",
    "                states.append(copy.deepcopy(state))\n",
    "                break\n",
    "\n",
    "            r1, c1, r2, c2 = move\n",
    "            state = self.apply_move(state, (r1, c1), (r2, c2))\n",
    "            states.append(copy.deepcopy(state))\n",
    "\n",
    "        if render:\n",
    "            return self._render(states)\n",
    "        elif \"winner\" in states[-1]:\n",
    "                winner = states[-1][\"winner\"]\n",
    "                if winner == 1:\n",
    "                    print(\"Game Over: Player Wins\")\n",
    "                elif winner == -1:\n",
    "                    print(\"Game Over: Opponent Wins\")\n",
    "                else:\n",
    "                    print(\"Game Over: Draw\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69341fd0",
   "metadata": {},
   "source": [
    "## Agents\n",
    "Different algorithms that interact with the environment are called agents. They all inherit from an `Agent` class, and must have a `make_move` function. We will implement a random moving agent as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f4019a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def make_move(self, state):\n",
    "        pass\n",
    "\n",
    "class RandomAgent(Agent):\n",
    "    def __init__(self, game: Onitama):\n",
    "        self.game = game\n",
    "    \n",
    "    def make_move(self, state):\n",
    "        moves = self.game.generate_moves(state)\n",
    "        if len(moves) > 0:\n",
    "            return random.choice(moves)\n",
    "        else:\n",
    "            return (0, 0, 0, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7acd868",
   "metadata": {},
   "source": [
    "### Minimax Agent\n",
    "Now your task is to implement minimax agents, once as a simple minimax algorithm and the second time with alpha-beta pruning. These functions from the game might prove useful to your implementation:\n",
    "- `generate_moves(state)`: Takes a state and returns a list of all possible moves.\n",
    "- `terminal(state)`: Returns `terminal, winner`, a boolean showing if the state is a terminal state, and if it is who is the winner (-1=opponent, 1=us, 0=draw)\n",
    "- `apply_move(state, from, to)`: Takes a move by its start and end, then implements it on the state. You can use it anytime as it effects the state not the game itself.\n",
    "- `run(agent1, agent2)`: Takes two agents and simulates a game between them, with the option to render the result as an animation.\n",
    "- `evaluate(state)`: Returns an integer to evaluate how good or bad the state is. You can use it to do a shallow minimax search and terminate when reaching a certain depth.\n",
    "- `get_pieces(board, player)`: Returns a list of all your pieces' information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7a5e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Minimax(Agent):\n",
    "    def __init__(self, game: Onitama, maximizing, depth):\n",
    "        self.game = game\n",
    "        self.maximizing = maximizing\n",
    "        self.depth = depth\n",
    "\n",
    "    def make_move(self, state):\n",
    "        _, move = self.minimax(state, self.depth, self.maximizing)\n",
    "        return move\n",
    "\n",
    "    def minimax(self, state, depth, maximizing):\n",
    "        # TODO\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60392132",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlphaBeta(Agent):\n",
    "    def __init__(self, game: Onitama, maximizing, depth):\n",
    "        self.game = game\n",
    "        self.maximizing = maximizing\n",
    "        self.depth = depth\n",
    "\n",
    "    def make_move(self, state):\n",
    "        _, move = self.alphabeta(state, self.depth, float('-inf'), float('inf'), self.maximizing)\n",
    "        return move\n",
    "\n",
    "    def alphabeta(self, state, depth, alpha, beta, maximizing):\n",
    "        # TODO\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9bdcd5",
   "metadata": {},
   "source": [
    "## Playground\n",
    "In this part you can make different agents with different settings, and simulate the result of their battle. You can also specify the game's settings as you desire!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f36ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "onitama_settings = {\n",
    "    \"board_size\": 5, \n",
    "    \"my_symbol\": 'o', \n",
    "    \"opponent_symbol\": 'x', \n",
    "    \"card1\": \"Tiger\", # Your starting card\n",
    "    \"card2\": \"Crab\", # Opponent starting card\n",
    "    \"card3\": \"Elephant\", # Center card\n",
    "    \"figsize\": (9, 6), # Changes the animation size\n",
    "    \"render_interval\": 800 # Changes the time between frames           \n",
    "}\n",
    "\n",
    "game = Onitama(**onitama_settings)\n",
    "\n",
    "agent1 = RandomAgent(game)\n",
    "agent2 = RandomAgent(game)\n",
    "\n",
    "# Other options\n",
    "# agent1 = Minimax(game, True, 5)\n",
    "# agent2 = AlphaBeta(game, False, 3)\n",
    "\n",
    "game.run(agent1, agent2, render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b324b29",
   "metadata": {},
   "source": [
    "# MDP\n",
    "\n",
    "Markov Decision Processes (MDPs) provide a powerful framework for modeling and analyzing real-world problems. An MDP is defined by a set of states, a set of actions, a transition function `T(s, a, s')` that gives the probability of transitioning from state s to state s' when taking action a, and a reward function `R(s, a, s')` that specifies the reward received for making that transition.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f666992c",
   "metadata": {},
   "source": [
    "## Scotland Yard\n",
    "\n",
    "**Scotland Yard** is a classic board game of cat and mouse. Several police officers work together to try to capture a thief known as **Mr. X**. \n",
    "\n",
    "At each turn, both the police officers and Mr. X move to different parts of London, choosing from three possible modes of transportation: `Taxi`, `Bus`, or a `Tunnel`. \n",
    "\n",
    "Mr. X loses if he is caught by one of the officers, but wins if he manages to evade capture and escape for long enough without being found.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082572ec",
   "metadata": {},
   "source": [
    "### Simplifications\n",
    "\n",
    "In the implemented version, several simplifications are applied:\n",
    "- There is only one police officer, which you control.\n",
    "- Mr. X moves randomly and cannot escape the city.\n",
    "- In every city (node), all three transportation options—Taxi, Bus, and Tunnel—are always available.\n",
    "- The current position of Mr. X is known at all times.\n",
    "\n",
    "---\n",
    "\n",
    "### Game Rules\n",
    "\n",
    "The police officer and Mr. X move along the nodes of a graph. At each turn, the officer can choose one of three actions: `Taxi`, `Bus`, or `Tunnel`. Each node has exactly one neighboring node associated with each action.\n",
    "\n",
    "When the officer attempts a move:\n",
    "- With some probability, the intended move is successful, and the officer follows the corresponding path.\n",
    "- If the move fails, the officer randomly takes one of the two alternative paths, each with equal probability.\n",
    "\n",
    "After the officer moves, Mr. X may make a move with some probability. If Mr. X decides to move, he randomly selects one of the three available paths from his current location. Otherwise, he remains in place.\n",
    "\n",
    "If **after both actions** the officer and Mr. X are at the same node, the officer wins and receives the `winning reward`. Otherwise, a `living reward` is applied, and the game continues.\n",
    "\n",
    "The move success probabilities for each action, along with the movement costs and Mr. X's behavior probabilities, can be customized to different values.\n",
    "\n",
    "---\n",
    "\n",
    "Most of the environment has been implemented below—except for the transition matrix `T` and the reward matrix `R`.  \n",
    "Your task is to complete the final function of the environment class, calculating `T` and `R` based purely on the environment's definition, **without taking samples from the environment**.\n",
    "\n",
    "You can run the cell after this for a visualization of the game. You can also use `self.state_index`, `self.index_state` and `self.action_index` to convert each actions and states to integers representing them in the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2bca1b",
   "metadata": {},
   "source": [
    "\n",
    "![Scotland Yard](ScotlandYard.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2863e681",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScotlandYard:\n",
    "    def __init__(\n",
    "            self,\n",
    "            police_start=\"A\",\n",
    "            x_start=\"J\",\n",
    "            move_probs=[0.9, 0.9, 0.9],\n",
    "            x_stay_prob=0.1,\n",
    "            win_reward=5,\n",
    "            living_reward=-1,\n",
    "            moving_rewards=[0, 0, 0],\n",
    "            render_interval=1000,\n",
    "            figsize=(6, 6),\n",
    "            color_map={\"taxi\": \"gold\", \"bus\": \"blue\", \"tunnel\": \"black\"},\n",
    "            ):\n",
    "\n",
    "        self.police_start = police_start\n",
    "        self.x_start = x_start\n",
    "        self.move_probs = move_probs\n",
    "        self.x_stay_prob = x_stay_prob\n",
    "        self.win_reward = win_reward\n",
    "        self.living_reward = living_reward\n",
    "        self.moving_rewards = moving_rewards\n",
    "        self.interval = render_interval\n",
    "        self.figsize = figsize\n",
    "        self.color_map = color_map\n",
    "\n",
    "        self.actions = [\"bus\", \"taxi\" , \"tunnel\"]\n",
    "        self.city_graph = {\n",
    "            \"A\": {\"taxi\": \"B\", \"bus\": \"C\", \"tunnel\": \"D\"},\n",
    "            \"B\": {\"taxi\": \"A\", \"bus\": \"F\", \"tunnel\": \"C\"},\n",
    "            \"C\": {\"taxi\": \"D\", \"bus\": \"A\", \"tunnel\": \"B\"},\n",
    "            \"D\": {\"taxi\": \"C\", \"bus\": \"E\", \"tunnel\": \"A\"},\n",
    "            \"E\": {\"taxi\": \"H\", \"bus\": \"D\", \"tunnel\": \"I\"},\n",
    "            \"F\": {\"taxi\": \"G\", \"bus\": \"B\", \"tunnel\": \"J\"},\n",
    "            \"G\": {\"taxi\": \"F\", \"bus\": \"I\", \"tunnel\": \"H\"},\n",
    "            \"H\": {\"taxi\": \"E\", \"bus\": \"J\", \"tunnel\": \"G\"},\n",
    "            \"I\": {\"taxi\": \"J\", \"bus\": \"G\", \"tunnel\": \"E\"},\n",
    "            \"J\": {\"taxi\": \"I\", \"bus\": \"H\", \"tunnel\": \"F\"},\n",
    "        }\n",
    "        self.cities = list(self.city_graph.keys())\n",
    "        self.history = []\n",
    "        self.states = [(p, x) for p in self.cities for x in self.cities]\n",
    "        self.state_index = {s: i for i, s in enumerate(self.states)}\n",
    "        self.index_state = {i: s for i, s in enumerate(self.states)}\n",
    "        self.action_index = {a: i for i, a in enumerate(self.actions)}\n",
    "        self._init_T_and_R()\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self, random_start=True):\n",
    "        if random_start:\n",
    "            self.police_pos, self.x_pos = random.sample(self.cities, 2)\n",
    "        else:\n",
    "            self.police_pos = self.police_start\n",
    "            self.x_pos = self.x_start\n",
    "        self.done = False\n",
    "        self.history = [(self.police_pos, self.x_pos)]\n",
    "        return (self.police_pos, self.x_pos)\n",
    "\n",
    "    def get_neighbors(self, pos):\n",
    "        return list(set(self.city_graph[pos].values()))\n",
    "\n",
    "    def move_mr_x(self):\n",
    "        if random.random() < self.x_stay_prob:\n",
    "            return self.x_pos\n",
    "        neighbors = self.get_neighbors(self.x_pos)\n",
    "        return random.choice(neighbors)\n",
    "\n",
    "    def step(self, action):\n",
    "        if self.done:\n",
    "            raise ValueError(\"Game has ended. Call reset() to restart.\")\n",
    "\n",
    "        success = random.random() < self.move_probs[self.actions.index(action)]\n",
    "        if success:\n",
    "            new_police_pos = self.city_graph[self.police_pos][action]\n",
    "        else:\n",
    "            fallback_options = [a for a in self.actions if a != action and a in self.city_graph[self.police_pos]]\n",
    "            if fallback_options:\n",
    "                new_action = random.choice(fallback_options)\n",
    "                new_police_pos = self.city_graph[self.police_pos][new_action]\n",
    "            else:\n",
    "                new_police_pos = self.police_pos\n",
    "\n",
    "        new_x_pos = self.move_mr_x()\n",
    "\n",
    "        self.police_pos = new_police_pos\n",
    "        self.x_pos = new_x_pos\n",
    "        self.history.append((self.police_pos, self.x_pos))\n",
    "\n",
    "        if self.police_pos == self.x_pos:\n",
    "            self.done = True\n",
    "            reward = self.win_reward\n",
    "        else:\n",
    "            reward = self.living_reward\n",
    "\n",
    "        reward += self.moving_rewards[self.actions.index(action)]\n",
    "\n",
    "        return (self.police_pos, self.x_pos), reward, self.done\n",
    "\n",
    "    def render(self):\n",
    "        G = nx.Graph()\n",
    "        edge_colors = []\n",
    "\n",
    "        for node, transports in self.city_graph.items():\n",
    "            for mode, neighbor in transports.items():\n",
    "                if G.has_edge(node, neighbor):\n",
    "                    continue\n",
    "                G.add_edge(node, neighbor, transport=mode)\n",
    "                edge_colors.append(self.color_map[mode])\n",
    "\n",
    "        pos = nx.spring_layout(G, seed=23)\n",
    "        fig, ax = plt.subplots(figsize=self.figsize)\n",
    "\n",
    "        def update(frame):\n",
    "            ax.clear()\n",
    "\n",
    "            transport_labels = nx.get_edge_attributes(G, \"transport\")\n",
    "            edge_colors_frame = [self.color_map[transport_labels[e]] for e in G.edges()]\n",
    "            nx.draw(G, pos, ax=ax, with_labels=True, node_color='lightblue',\n",
    "                    edge_color=edge_colors_frame, node_size=500, font_size=10, width=2)\n",
    "\n",
    "            police, x = self.history[frame]\n",
    "\n",
    "            nx.draw_networkx_nodes(G, pos, nodelist=[police], node_color='green', node_size=600, ax=ax)\n",
    "            nx.draw_networkx_nodes(G, pos, nodelist=[x], node_color='red', node_size=600, ax=ax)\n",
    "\n",
    "            ax.set_title(f\"Step {frame+1}: Police at {police}, Mr. X at {x}\")\n",
    "\n",
    "        ani = animation.FuncAnimation(fig, update, frames=len(self.history), interval=self.interval, repeat=False)\n",
    "        plt.close(fig)\n",
    "        return HTML(ani.to_jshtml())\n",
    "\n",
    "    def _init_T_and_R(self):\n",
    "        S = len(self.states)\n",
    "        A = len(self.actions)\n",
    "        self.T = np.zeros((S, A, S))\n",
    "        self.R = np.zeros((S, A, S))\n",
    "\n",
    "        # TODO: calculate and replace the actual values for the two matrices\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffb2433",
   "metadata": {},
   "source": [
    "You can modify the environment settings here. Make sure to run the game with different numbers and observe how the result changes according to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bd317f",
   "metadata": {},
   "outputs": [],
   "source": [
    "scotlandyard_settings = {\n",
    "    \"police_start\": \"A\",\n",
    "    \"x_start\": \"J\",\n",
    "    \"move_probs\": [0.7, 0.8, 0.9],\n",
    "    \"x_stay_prob\": 0.1,\n",
    "    \"win_reward\": 50,\n",
    "    \"living_reward\": -10,\n",
    "    \"moving_rewards\": [1, 2, 3],\n",
    "    \"render_interval\": 1000,\n",
    "    \"figsize\": (6, 6),\n",
    "    \"color_map\": {\"taxi\": \"gold\", \"bus\": \"blue\", \"tunnel\": \"black\"},\n",
    "}\n",
    "\n",
    "env = ScotlandYard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3824dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "for _ in range(15):\n",
    "    env.step(random.choice(env.actions))\n",
    "    if env.done:\n",
    "        break\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52676346",
   "metadata": {},
   "source": [
    "## Validating T and R\n",
    "Now you have to validate your calculated results by sampling many episodes and estimating the T and R values based on them. \n",
    "\n",
    "The environment is implemented like a `gym` environment (you will work with them in the next section). The environment goes back to its initial state by calling `env.reset()`, you can make an action in the environment using `env.step()` and rendering will be done using `env.render()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3024315",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(env: ScotlandYard, num_episodes=1000000, max_steps_per_episode=30):\n",
    "    S = len(env.states)\n",
    "    A = len(env.actions)\n",
    "    T_hat = np.zeros((S, A, S))\n",
    "    R_hat = np.zeros((S, A, S))\n",
    "\n",
    "    # TODO\n",
    "\n",
    "    return T_hat, R_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1515e3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure to initialize the environment again in order to apply your implemented T and R\n",
    "env.reset()\n",
    "T_hat, R_hat = validate(env)\n",
    "\n",
    "print(f\"T error: {norm(env.T - T_hat):.4f}\")\n",
    "print(f\"R error: {norm(env.R - R_hat):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6567c4e7",
   "metadata": {},
   "source": [
    "## Value Iteration\n",
    "Now you can use your implemented `T` and `R` to run the value iteration algorithm to estimate the `V` values for this environment. Complete the following code. It should run the algorithm until the difference between two consecutive `V`s is less that theta. The next cell will run the game according to your policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367e354c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env: ScotlandYard, discount=0.95, theta=1e-6):\n",
    "    S, A, _ = env.T.shape\n",
    "    V = np.zeros(S)\n",
    "    policy = np.zeros(S, dtype=int)\n",
    "\n",
    "    # TODO\n",
    "\n",
    "    return V, policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd84cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure to initialize the environment again in order to apply your implemented T and R\n",
    "env.reset()\n",
    "_, policy = value_iteration(env)\n",
    "state = env.reset()\n",
    "for _ in range(30):\n",
    "    action = policy[env.state_index[state]]\n",
    "    state, _, done = env.step(env.actions[action])\n",
    "    if done:\n",
    "        break\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554de439",
   "metadata": {},
   "source": [
    "# RL\n",
    "\n",
    "Reinforcement Learning (RL) is a family of algorithms designed to learn by interacting with an environment. It focuses on improving a policy over time through trial and error, aiming to make better decisions based on experience and feedback. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6902f707",
   "metadata": {},
   "source": [
    "## Gym\n",
    "\n",
    "One of the most popular frameworks for developing and testing Reinforcement Learning (RL) algorithms is **Gym**. Originally developed by OpenAI and now maintained as **Gymnasium**, it provides a simple and standardized interface for a wide variety of environments—from classic control problems like CartPole to more complex tasks like robotic manipulation and board games.\n",
    "\n",
    "In Gym, an environment defines:\n",
    "- **State Space**: What the agent observes.\n",
    "- **Action Space**: The set of actions the agent can take.\n",
    "- **Transition Dynamics**: How the environment responds to actions.\n",
    "- **Rewards**: The feedback signal used to guide learning.\n",
    "- **Episode Termination**: Conditions under which an episode ends.\n",
    "\n",
    "The typical workflow involves:\n",
    "1. **Resetting** the environment to get the initial observation.\n",
    "2. **Taking actions** based on a policy.\n",
    "3. **Observing** the next state and receiving a reward.\n",
    "4. **Repeating** until the episode terminates.\n",
    "\n",
    "Gym makes it easy to focus on developing and testing RL algorithms without having to build environments from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c651c657",
   "metadata": {},
   "source": [
    "## CartPole\n",
    "\n",
    "A classic example environment in Gym is **CartPole**. It is often used as a simple benchmark for testing basic RL algorithms.\n",
    "\n",
    "In CartPole, the agent controls a cart that can move left or right along a track, with a pole attached to its top. The objective is to keep the pole balanced upright for as long as possible. \n",
    "\n",
    "The environment provides:\n",
    "- **State**: A 4-dimensional vector consisting of:\n",
    "  - Cart position\n",
    "  - Cart velocity\n",
    "  - Pole angle\n",
    "  - Pole angular velocity\n",
    "- **Actions**: Two discrete actions:\n",
    "  - Move cart left (`0`)\n",
    "  - Move cart right (`1`)\n",
    "- **Rewards**: The agent receives a reward of `+1` for every timestep that the pole remains upright.\n",
    "- **Episode Termination**: The episode ends if:\n",
    "  - The pole falls beyond a certain angle.\n",
    "  - The cart moves too far off the center of the track.\n",
    "  - A maximum number of steps is reached.\n",
    "\n",
    "CartPole is simple enough to allow for rapid experimentation, yet rich enough to demonstrate core concepts in reinforcement learning such as balance between exploration and exploitation, value estimation, and policy improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c884d6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CartPoleEnv(Env):\n",
    "    metadata = {'render_modes': ['rgb_array']}\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.gravity = 9.8\n",
    "        self.masscart = 1.0\n",
    "        self.masspole = 0.1\n",
    "        self.total_mass = self.masscart + self.masspole\n",
    "        self.length = 0.5\n",
    "        self.polemass_length = self.masspole * self.length\n",
    "        self.force_mag = 10.0\n",
    "        self.tau = 0.02\n",
    "\n",
    "        self.theta_threshold_radians = 12 * 2 * np.pi / 360\n",
    "        self.x_threshold = 2.4\n",
    "\n",
    "        high = np.array([\n",
    "            self.x_threshold * 2,\n",
    "            np.finfo(np.float32).max,\n",
    "            self.theta_threshold_radians * 2,\n",
    "            np.finfo(np.float32).max\n",
    "        ], dtype=np.float32)\n",
    "\n",
    "        self.observation_space = spaces.Box(-high, high, dtype=np.float32)\n",
    "        self.action_space = spaces.Discrete(2)  # 0 = push left, 1 = push right\n",
    "\n",
    "        self.state = None\n",
    "        self.steps_beyond_done = None\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.state = self.np_random.uniform(low=-0.05, high=0.05, size=(4,))\n",
    "        self.steps_beyond_done = None\n",
    "        return self.state, {}\n",
    "\n",
    "    def step(self, action):\n",
    "        x, x_dot, theta, theta_dot = self.state\n",
    "        force = self.force_mag if action == 1 else -self.force_mag\n",
    "\n",
    "        costheta = np.cos(theta)\n",
    "        sintheta = np.sin(theta)\n",
    "\n",
    "        temp = (force + self.polemass_length * theta_dot**2 * sintheta) / self.total_mass\n",
    "        thetaacc = (self.gravity * sintheta - costheta * temp) / \\\n",
    "                   (self.length * (4.0/3.0 - self.masspole * costheta**2 / self.total_mass))\n",
    "        xacc = temp - self.polemass_length * thetaacc * costheta / self.total_mass\n",
    "\n",
    "        x = x + self.tau * x_dot\n",
    "        x_dot = x_dot + self.tau * xacc\n",
    "        theta = theta + self.tau * theta_dot\n",
    "        theta_dot = theta_dot + self.tau * thetaacc\n",
    "\n",
    "        self.state = (x, x_dot, theta, theta_dot)\n",
    "\n",
    "        done = bool(\n",
    "            x < -self.x_threshold or x > self.x_threshold or\n",
    "            theta < -self.theta_threshold_radians or theta > self.theta_threshold_radians\n",
    "        )\n",
    "\n",
    "        reward = 1.0 if not done else 0.0\n",
    "        return np.array(self.state, dtype=np.float32), reward, done, False, {}\n",
    "\n",
    "    def render(self, mode='rgb_array'):\n",
    "        screen_width = 608\n",
    "        screen_height = 400\n",
    "\n",
    "        world_width = self.x_threshold * 2\n",
    "        scale = screen_width / world_width\n",
    "        carty = 100\n",
    "        polewidth = 10.0\n",
    "        polelen = scale * (2 * self.length)\n",
    "        cartwidth = 50.0\n",
    "        cartheight = 30.0\n",
    "\n",
    "        x = self.state[0]\n",
    "        theta = self.state[2]\n",
    "\n",
    "        cartx = x * scale + screen_width / 2.0\n",
    "        top = carty - cartheight / 2\n",
    "        bottom = carty + cartheight / 2\n",
    "        left = cartx - cartwidth / 2\n",
    "        right = cartx + cartwidth / 2\n",
    "\n",
    "        image = Image.new(\"RGB\", (screen_width, screen_height), (255, 255, 255))\n",
    "        draw = ImageDraw.Draw(image)\n",
    "\n",
    "        draw.rectangle([left, top, right, bottom], fill=(0, 0, 0))\n",
    "\n",
    "        pole_x1 = cartx\n",
    "        pole_y1 = carty\n",
    "        pole_x2 = cartx + polelen * np.sin(theta)\n",
    "        pole_y2 = carty - polelen * np.cos(theta)\n",
    "        draw.line([pole_x1, pole_y1, pole_x2, pole_y2], fill=(255, 0, 0), width=5)\n",
    "\n",
    "        if mode == 'rgb_array':\n",
    "            return np.array(image)\n",
    "        elif mode == 'human':\n",
    "            image.show()\n",
    "\n",
    "    def close(self):\n",
    "        pass\n",
    "\n",
    "def record_video_cartpole(env: CartPoleEnv, agent, out_dir=\"video\", fps=30, max_steps=1000):\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    frames = []\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    for _ in range(max_steps):\n",
    "        state = agent.obs_to_state(obs)\n",
    "        action = agent.act(state, greedy=True)\n",
    "        obs, reward, done, _, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        img = env.render()\n",
    "        frames.append(img)\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    path = os.path.join(out_dir, \"cartpole.mp4\")\n",
    "    imageio.mimsave(path, frames, fps=fps)\n",
    "    return total_reward, path\n",
    "\n",
    "def show_video(video_path, width=400):\n",
    "    with open(video_path, \"rb\") as f:\n",
    "        encoded = b64encode(f.read()).decode()\n",
    "    return HTML(f'<video width=\"{width}\" controls><source src=\"data:video/mp4;base64,{encoded}\" type=\"video/mp4\">')\n",
    "\n",
    "def plot_rewards(rewards, window=50):\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(rewards, label='Episode reward')\n",
    "    if len(rewards) >= window:\n",
    "        moving_avg = np.convolve(rewards, np.ones(window) / window, mode='valid')\n",
    "        plt.plot(range(window - 1, len(rewards)), moving_avg, label=f'{window}-episode moving avg', linewidth=2)\n",
    "    plt.title(\"Training Reward over Episodes\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Total Reward\")\n",
    "    # plt.ylim(0, 1000)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039ef105",
   "metadata": {},
   "source": [
    "## Q-Learning\n",
    "\n",
    "Now, you will implement the **Q-Learning** algorithm to solve the **CartPole** environment. \n",
    "\n",
    "However, there is a major challenge: the CartPole environment has a **continuous** state space, while Q-Learning requires **discrete** states to store values in a Q-table. \n",
    "\n",
    "To overcome this, we need to **discretize** the state space by dividing each continuous state variable into a finite number of intervals (also called \"bins\"). Each bin will represent a range of continuous values and will be treated as a distinct discrete state.\n",
    "\n",
    "This discretization allows us to approximate the continuous environment using a manageable Q-table, making it possible to apply the standard Q-Learning algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd614dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    def __init__(self, env: CartPoleEnv, number_states_for_feature, gamma, alpha, alpha_min, alpha_max, epsilon, epsilon_min, epsilon_max):\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.state_bounds = list(zip(self.env.observation_space.low, self.env.observation_space.high))\n",
    "        self.state_bounds[1] = [-0.5, 0.5]\n",
    "        self.state_bounds[3] = [-math.radians(50), math.radians(50)]\n",
    "\n",
    "        self.number_states_for_feature = number_states_for_feature\n",
    "        np.random.seed(42)\n",
    "        random.seed(42)\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_max = epsilon_max\n",
    "        self.alpha = alpha\n",
    "        self.alpha_min = alpha_min\n",
    "        self.alpha_max = alpha_max\n",
    "        self.Q_table = {}\n",
    "\n",
    "    def obs_to_state(self, observation):\n",
    "        states_list = []\n",
    "        for i in range(len(observation)):\n",
    "            if observation[i] <= self.state_bounds[i][0]:\n",
    "                state_index = 0\n",
    "            elif observation[i] >= self.state_bounds[i][1]:\n",
    "                state_index = self.number_states_for_feature[i] - 1\n",
    "            else:\n",
    "                quantum_size = (self.state_bounds[i][1] - self.state_bounds[i][0]) / self.number_states_for_feature[i]\n",
    "                offset = observation[i] - self.state_bounds[i][0]\n",
    "                state_index = math.floor(offset / quantum_size)\n",
    "            states_list.append(state_index)\n",
    "        return tuple(states_list)\n",
    "\n",
    "    def act(self, state, greedy=False):\n",
    "        if not greedy and random.random() < self.epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        \n",
    "        # TODO\n",
    "\n",
    "    def schedule_epsilon(self, t):\n",
    "        # TODO: set self.epsilon based on your scheduler\n",
    "        pass\n",
    "\n",
    "    def schedule_alpha(self, t):\n",
    "        # TODO: set self.alpha based on your scheduler\n",
    "        pass\n",
    "\n",
    "    def train(self, episodes=1000):\n",
    "        rewards = []\n",
    "        for ep in range(episodes):\n",
    "            self.schedule_epsilon(ep)\n",
    "            self.schedule_alpha(ep)\n",
    "\n",
    "            obs, _ = self.env.reset()\n",
    "            total_reward = 0\n",
    "            done = False\n",
    "\n",
    "            # TODO\n",
    "\n",
    "            while not done:\n",
    "                next_obs, reward, done, _, _ = self.env.step(action)\n",
    "\n",
    "                # TODO\n",
    "\n",
    "                total_reward += reward\n",
    "\n",
    "            rewards.append(total_reward)\n",
    "\n",
    "            if (ep + 1) % 100 == 0:\n",
    "                print(f\"Episode {ep+1}: Reward = {total_reward}, Epsilon = {self.epsilon:.3f}\")\n",
    "        return rewards\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491c892d",
   "metadata": {},
   "source": [
    "Set the hyperparameters of your model! Epsilon is the probability for taking a random action in epsilon greedy algorithm, alpha is the learning rate for adding the new observations to your q table, and gamma is the discount factor for the rewards. Then, train your agent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c89923a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode=\"rgb_array\")\n",
    "\n",
    "# TODO\n",
    "qlearning_hyperparameters = {\n",
    "    \"env\": env,\n",
    "    \"number_states_for_feature\": (1, 1, 1, 1), # Choose the number of intervals for each state\n",
    "    \"gamma\": -1,\n",
    "    \"alpha\": -1, \n",
    "    \"alpha_min\": -1,\n",
    "    \"alpha_max\": -1, \n",
    "    \"epsilon\": -1, \n",
    "    \"epsilon_min\": -1, \n",
    "    \"epsilon_max\": -1,\n",
    "}\n",
    "\n",
    "agent = QLearningAgent(**qlearning_hyperparameters)\n",
    "rewards = agent.train(episodes=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042a8913",
   "metadata": {},
   "source": [
    "Plot the rewards history. The moving average of the rewards must be going up, meaning that your agent is improving!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3de98f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rewards(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdc3947",
   "metadata": {},
   "source": [
    "Now render your model's best performance based on your q table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a4ceb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, path = record_video_cartpole(env, agent)\n",
    "show_video(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba22a76",
   "metadata": {},
   "source": [
    "## Direct Evaluation\n",
    "\n",
    "Now, you will perform **Direct Evaluation** to estimate the state-value function `V(s)`of your environment under the policy learned by your Q-Learning agent.\n",
    "\n",
    "In Direct Evaluation, you use **sampling**: by running episodes following the learned policy, you collect returns (cumulative rewards) and use them to estimate the expected value of each state. \n",
    "\n",
    "Specifically, for each state `s`, you estimate `V(s)` as the average of the total rewards observed starting from that state, following the current policy.\n",
    "\n",
    "This method allows you to approximate the true value function without requiring access to the full model of the environment.\n",
    "\n",
    "You must first calculate the real `V` from the agent's learned q table, then use the above method to estimate `V` to evaluate the model's quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5469ea55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(env: CartPoleEnv, agent: QLearningAgent, gamma=0.99, episodes=10000):\n",
    "    V = {}\n",
    "    V_hat = {}\n",
    "    \n",
    "    # TODO\n",
    "\n",
    "    return np.linalg.norm([V[a] - V_hat[a] for a in V.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0c69a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(env, agent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
